{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805649d0-d648-404f-8e5d-0681d5f0ad04",
   "metadata": {},
   "source": [
    "# Importer les librairies et les classes nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abef9211-d7e9-4e5b-aeef-fbbee960b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import urllib, urllib.request\n",
    "import xmltodict\n",
    "import datetime\n",
    "import pickle\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from Classes import Document, RedditDocument, ArxivDocument\n",
    "from Classes import Document\n",
    "from Classes import Author\n",
    "from Corpus import Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7984ab-9197-49ab-bd40-4d61127c477e",
   "metadata": {},
   "source": [
    "# Fonction pour afficher la structure hiérarchique d'un dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c06368-8ae0-4a69-b5f0-15097c1af928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showDictStruct(d):\n",
    "    def recursivePrint(d, i):\n",
    "        for k in d:\n",
    "            if isinstance(d[k], dict):\n",
    "                print(\"-\"*i, k)\n",
    "                recursivePrint(d[k], i+2)\n",
    "            else:\n",
    "                print(\"-\"*i, k, \":\", d[k])\n",
    "    recursivePrint(d, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7615a-fd78-4714-b206-3178b4329313",
   "metadata": {},
   "source": [
    "# Configuration de l'API Reddit à l'aide la bibliothèque praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c688f1d-22bd-4c9c-9a5d-0a388a72894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='wfrwVmRrikEa9yhkXWMzMQ', client_secret='ahqeIx-Xp4pbFgGccGsLrrd1aecWkg', user_agent='projet_python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c5f12-3f73-4355-8707-b6bf97363911",
   "metadata": {},
   "source": [
    "## Récupération des 100 publications populaires sur Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58386a52-be37-407d-b59d-03a8ebb3b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100\n",
    "hot_posts = reddit.subreddit('all').hot(limit=limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc081e2-69e7-4c13-98db-db23ed2100a5",
   "metadata": {},
   "source": [
    "### Récupération des données à partir de ces 100 publications populaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d74d2f3-cc17-4f89-8b81-e8587f64fb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit: 0 / 100\n",
      "Reddit: 10 / 100\n",
      "Reddit: 20 / 100\n",
      "Reddit: 30 / 100\n",
      "Reddit: 40 / 100\n",
      "Reddit: 50 / 100\n",
      "Reddit: 60 / 100\n",
      "Reddit: 70 / 100\n",
      "Reddit: 80 / 100\n",
      "Reddit: 90 / 100\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Shes slowly turning orange', '', '', '', '', '', '', '', '', '', '', '', 'From First To Last had just dropped \"Dear diary...\"  and every weekend was another alternative live show. For every cringe, there is also a great memory.', '', '', '', '', '', '', '', '', '', 'Professor will be responding to my discussion board posts and I will have to respond back to get full credit.   No problem.', '', '', 'We booked the tickets and we ended up not including the daughters mainly because they just kept complaining.  Last night they came to ask me why I wouldn\\'t buy it if I can\\'t afford it and why can\\'t they stay home alone. They said \"why don\\'t you guys care about our comforts and privacy\" and \"you\\'ll ruin the trip because we won\\'t even have our own rooms which is a basic comfort\".   They tried to convince us to let us stay home alone but the last time I did that my oldest threw a party so I don\\'t trust them and even if I did I don\\'t feel comfortable leaving them alone for a whole week.  They threw another tantrum and I just told them they wouldn\\'t be coming and it was final.   I was going to leave them at my MIL but she wants to take our daughters on a 4 day trip to Sydney where they will get their own rooms she did this \"make up for us ruining the daughters trip by making them share a room and excluding them if they didn\\'t\" but I just told her no because I\\'ve spoiled them too much and I\\'m not rewarding a tantrum.   My sister didn\\'t want to take them but my brother agreed to take them so I\\'ll  be leaving them there where one of them has to sleep on the couch and the other on an air mattes as they don\\'t have an extra room.  My daughters have complained but they aren\\'t coming   This will probably be my final update thanks.', '', '', '', '', '', '', '', '', '“She said she thought the cocktail was called \"1890,\" and she assumed that was the price of the cocktail as well — £18.90 (or about $24 in U.S. dollars). The cocktail contained Cristal, an expensive brand of champagne, as well as \"30-year-old cognac\" and gold leaf, she said.   She also said she was given a \"huge book to sign.\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Credit: @hassankhadair', '', '', '', '', '', '', '', \"I knew that if my husband called and basically said what I had said, he'd be listened to. In the mean time I lay in bed almost unable to move from the pain (I had deeply infiltrated endometriosis  excised).  She was so caring to me once my husband was in the room that I felt like crying. She told me to 'not let my pain get to more than 5 out of 10 on the pain scale'...after I told her it was an 8 today and she just walked off. I lay in agony for 3 hours before I tearfully called my husband and asked him to advocate for me. My husband gloomily said he thought the call made a big difference too.  I dont know what to say about it, I guess I'm just feeling very sad. It was pretty traumatic to be in that pain and have to problem solve how to get listened to.  I am very grateful to this subreddit because I think it's pushed me to do this and just wield my man when i need if I am in a desperate situation. I hate that I have to do it but unfortunately it's been effective a few times now.\", '', '', 'By the yt channel (Pertinax) ', '', '', '', '', '', 'Am I the only one or Am I dumb?', 'Source : https://www.instagram.com/reel/CzgK4iZqmsx/?igsh=MTA3YWM0MWlnZmNzZw==', 'Source:   https://twitter.com/MarkHamill/status/1744452324356190334?t=TIpJoBgIATZJIAlNEJilyw&s=19', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "docs_bruts = []\n",
    "afficher_cles = False\n",
    "for i, post in enumerate(hot_posts):\n",
    "    if i%10==0: print(\"Reddit:\", i, \"/\", limit)\n",
    "    # Pour connaître les différentes variables et leur contenu\n",
    "    if afficher_cles:  \n",
    "        for k, v in post.__dict__.items():\n",
    "            pass\n",
    "            print(k, \":\", v)\n",
    "\n",
    "    # On ne considère pas les posts sans texte\n",
    "    if post.selftext != \"\":  \n",
    "        pass\n",
    "    docs.append(post.selftext.replace(\"\\n\", \" \"))\n",
    "    docs_bruts.append((\"Reddit\", post))\n",
    "\n",
    "print (docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b63d4-59c8-4cbe-bd4a-871325e899af",
   "metadata": {},
   "source": [
    "# Récupération des résumés de publications ArXiv avec une requête API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3d31c0-b150-4fe2-94c5-426f9b815f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArXiv: 0 / 100\n",
      "ArXiv: 10 / 100\n",
      "ArXiv: 20 / 100\n",
      "ArXiv: 30 / 100\n",
      "ArXiv: 40 / 100\n"
     ]
    }
   ],
   "source": [
    "# Paramètres\n",
    "query_terms = [\"clustering\", \"Dirichlet\"]\n",
    "max_results = 50\n",
    "\n",
    "# Requête\n",
    "url = f'http://export.arxiv.org/api/query?search_query=all:{\"+\".join(query_terms)}&start=0&max_results={max_results}'\n",
    "data = urllib.request.urlopen(url)\n",
    "\n",
    "# Format dict (OrderedDict)\n",
    "data = xmltodict.parse(data.read().decode('utf-8'))\n",
    "\n",
    "#showDictStruct(data)\n",
    "\n",
    "# Ajout résumés à la liste\n",
    "for i, entry in enumerate(data[\"feed\"][\"entry\"]):\n",
    "    if i%10==0: print(\"ArXiv:\", i, \"/\", limit)\n",
    "    docs.append(entry[\"summary\"].replace(\"\\n\", \"\"))\n",
    "    docs_bruts.append((\"ArXiv\", entry))\n",
    "    #showDictStruct(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10c626-1bee-43cd-adbb-00d65c6a1956",
   "metadata": {},
   "source": [
    "# Prétraitement et exploitation des données de Reddit et Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01b78a6-54b0-4082-854b-c93e1bf78891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# docs avec doublons : 150\n",
      "# docs sans doublons : 62\n",
      "Document 0\t# caractères : 0\t# mots : 1\t# phrases : 1\n",
      "Document 1\t# caractères : 977\t# mots : 137\t# phrases : 8\n",
      "Document 2\t# caractères : 1006\t# mots : 134\t# phrases : 7\n",
      "Document 3\t# caractères : 31\t# mots : 9\t# phrases : 1\n",
      "Document 4\t# caractères : 949\t# mots : 125\t# phrases : 6\n",
      "Document 5\t# caractères : 756\t# mots : 100\t# phrases : 8\n",
      "Document 6\t# caractères : 1137\t# mots : 165\t# phrases : 7\n",
      "Document 7\t# caractères : 26\t# mots : 4\t# phrases : 1\n",
      "Document 8\t# caractères : 1023\t# mots : 132\t# phrases : 6\n",
      "Document 9\t# caractères : 513\t# mots : 63\t# phrases : 5\n",
      "Document 10\t# caractères : 1155\t# mots : 150\t# phrases : 8\n",
      "Document 11\t# caractères : 609\t# mots : 86\t# phrases : 5\n",
      "Document 12\t# caractères : 802\t# mots : 107\t# phrases : 7\n",
      "Document 13\t# caractères : 1033\t# mots : 129\t# phrases : 8\n",
      "Document 14\t# caractères : 1360\t# mots : 180\t# phrases : 7\n",
      "Document 15\t# caractères : 1339\t# mots : 276\t# phrases : 9\n",
      "Document 16\t# caractères : 1600\t# mots : 218\t# phrases : 11\n",
      "Document 17\t# caractères : 670\t# mots : 82\t# phrases : 5\n",
      "Document 18\t# caractères : 153\t# mots : 27\t# phrases : 6\n",
      "Document 19\t# caractères : 78\t# mots : 3\t# phrases : 3\n",
      "Document 20\t# caractères : 328\t# mots : 61\t# phrases : 7\n",
      "Document 21\t# caractères : 1831\t# mots : 211\t# phrases : 12\n",
      "Document 22\t# caractères : 984\t# mots : 121\t# phrases : 7\n",
      "Document 23\t# caractères : 1320\t# mots : 167\t# phrases : 8\n",
      "Document 24\t# caractères : 1817\t# mots : 246\t# phrases : 11\n",
      "Document 25\t# caractères : 418\t# mots : 63\t# phrases : 4\n",
      "Document 26\t# caractères : 1261\t# mots : 172\t# phrases : 11\n",
      "Document 27\t# caractères : 950\t# mots : 133\t# phrases : 7\n",
      "Document 28\t# caractères : 1100\t# mots : 146\t# phrases : 7\n",
      "Document 29\t# caractères : 950\t# mots : 129\t# phrases : 8\n",
      "Document 30\t# caractères : 1894\t# mots : 246\t# phrases : 10\n",
      "Document 31\t# caractères : 1013\t# mots : 133\t# phrases : 9\n",
      "Document 32\t# caractères : 123\t# mots : 24\t# phrases : 3\n",
      "Document 33\t# caractères : 1043\t# mots : 156\t# phrases : 9\n",
      "Document 34\t# caractères : 423\t# mots : 56\t# phrases : 4\n",
      "Document 35\t# caractères : 828\t# mots : 112\t# phrases : 6\n",
      "Document 36\t# caractères : 1201\t# mots : 162\t# phrases : 14\n",
      "Document 37\t# caractères : 1715\t# mots : 214\t# phrases : 9\n",
      "Document 38\t# caractères : 1196\t# mots : 158\t# phrases : 7\n",
      "Document 39\t# caractères : 1360\t# mots : 176\t# phrases : 10\n",
      "Document 40\t# caractères : 1247\t# mots : 149\t# phrases : 8\n",
      "Document 41\t# caractères : 1096\t# mots : 141\t# phrases : 8\n",
      "Document 42\t# caractères : 22\t# mots : 2\t# phrases : 1\n",
      "Document 43\t# caractères : 627\t# mots : 84\t# phrases : 7\n",
      "Document 44\t# caractères : 1096\t# mots : 157\t# phrases : 7\n",
      "Document 45\t# caractères : 97\t# mots : 4\t# phrases : 2\n",
      "Document 46\t# caractères : 1876\t# mots : 257\t# phrases : 11\n",
      "Document 47\t# caractères : 909\t# mots : 125\t# phrases : 6\n",
      "Document 48\t# caractères : 1029\t# mots : 136\t# phrases : 7\n",
      "Document 49\t# caractères : 997\t# mots : 148\t# phrases : 9\n",
      "Document 50\t# caractères : 1012\t# mots : 146\t# phrases : 7\n",
      "Document 51\t# caractères : 29\t# mots : 6\t# phrases : 1\n",
      "Document 52\t# caractères : 1315\t# mots : 197\t# phrases : 9\n",
      "Document 53\t# caractères : 394\t# mots : 55\t# phrases : 8\n",
      "Document 54\t# caractères : 969\t# mots : 124\t# phrases : 6\n"
     ]
    }
   ],
   "source": [
    "# Affichage et suppression des doublons\n",
    "print(f\"# docs avec doublons : {len(docs)}\")\n",
    "docs = list(set(docs))\n",
    "print(f\"# docs sans doublons : {len(docs)}\")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i}\\t# caractères : {len(doc)}\\t# mots : {len(doc.split(' '))}\\t# phrases : {len(doc.split('.'))}\")\n",
    "    if len(doc)<100:\n",
    "        docs.remove(doc)\n",
    "\n",
    "# Concaténation des documents restants\n",
    "longueChaineDeCaracteres = \" \".join(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8643f90-fd71-420b-99f4-ceb34716ba65",
   "metadata": {},
   "source": [
    "# Factory Pattern pour la création d'instances de documents à partir de données brutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e022677-43e8-4feb-b39c-ee318c9baa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = []\n",
    "# FACTORY PATTERN\n",
    "class DocumentFactory:\n",
    "    @staticmethod\n",
    "    def create_document(nature, doc):\n",
    "        if nature == \"ArXiv\":  # Les fichiers de ArXiv ou de Reddit ne sont pas formatés de la même manière à ce stade.\n",
    "        #showDictStruct(doc)\n",
    "\n",
    "            titre = doc[\"title\"].replace('\\n', '')  # On enlève les retours à la ligne\n",
    "            \"\"\"try:\n",
    "                authors = \", \".join([a[\"name\"] for a in doc[\"author\"]])  # On fait une liste d'auteurs, séparés par une virgule\n",
    "            except:\n",
    "                authors = doc[\"author\"][\"name\"]  # Si l'auteur est seul, pas besoin de liste\"\"\"\n",
    "            # récupérer l'auteur et les co-auteur\n",
    "            try:\n",
    "                authors_list = [a[\"name\"] for a in doc[\"author\"]]\n",
    "                primary_author = authors_list[0]\n",
    "                co_authors = authors_list[1:] if len(authors_list) > 1 else []\n",
    "            except TypeError:\n",
    "                primary_author = doc[\"author\"][\"name\"]\n",
    "                co_authors = []\n",
    "            summary = doc[\"summary\"].replace(\"\\n\", \"\")  # On enlève les retours à la ligne\n",
    "            date = datetime.datetime.strptime(doc[\"published\"], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y/%m/%d\")  # Formatage de la date en année/mois/jour avec librairie datetime\n",
    "\n",
    "            #return ArxivDocument(titre, authors, date, doc[\"id\"], summary) \n",
    "            return ArxivDocument(titre, primary_author, date, doc[\"id\"], summary, co_auteurs=co_authors)# Création du Document\n",
    "           \n",
    "        elif nature == \"Reddit\":\n",
    "            titre = doc.title.replace(\"\\n\", '')\n",
    "            auteur = str(doc.author)\n",
    "            date = datetime.datetime.fromtimestamp(doc.created).strftime(\"%Y/%m/%d\")\n",
    "            url = \"https://www.reddit.com/\"+doc.permalink\n",
    "            texte = doc.selftext.replace(\"\\n\", \"\")\n",
    "            nb_commentaires = doc.num_comments\n",
    "            return RedditDocument(titre, auteur, date, url, texte, nb_commentaires)\n",
    "        else:\n",
    "            raise ValueError(\"Nature de document non supportée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee8bff-0988-46a0-832c-717c147c5111",
   "metadata": {},
   "source": [
    "### Utilisation du factory pour créer des objets Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26537d43-87c7-4fba-8c32-d950c2a5f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nature, doc in docs_bruts:\n",
    "    doc_classe = DocumentFactory.create_document(nature, doc)\n",
    "    collection.append(doc_classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb3502-1fb7-407e-850e-379fc1c57e39",
   "metadata": {},
   "source": [
    "### Création de l'index de documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4c67df-c626-411b-a04c-6f0b4581d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2doc = {}\n",
    "for i, doc in enumerate(collection):\n",
    "    id2doc[i] = doc.titre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec7b56-3c97-4255-aaa2-fe85612115be",
   "metadata": {},
   "source": [
    "### Gestion des Auteurs et création d'une liste d'objets Auteur avec index associé\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255f6e07-4a28-4891-a932-ca80890a5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = {}\n",
    "aut2id = {}\n",
    "num_auteurs_vus = 0\n",
    "\n",
    "# Création de la liste et l'index des Auteurs\n",
    "for doc in collection:\n",
    "    if doc.auteur not in aut2id:\n",
    "        num_auteurs_vus += 1\n",
    "        authors[num_auteurs_vus] = Author(doc.auteur)\n",
    "        aut2id[doc.auteur] = num_auteurs_vus\n",
    "\n",
    "    authors[aut2id[doc.auteur]].add(doc.texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3afda45-fbe5-435e-ac99-0f8689cbea52",
   "metadata": {},
   "source": [
    "# Construction du corpus à partir des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e467b590-6292-404a-a816-69729eaff019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le corpus et le fichier CSV existent déjà.\n",
      "\"Dorohedoro\" sequel anime announced, par harushiga , type: Reddit\tNombre de commentaires : 836\n",
      "--title has been lost--, par eekfirebolt , type: Reddit\tNombre de commentaires : 68\n",
      "=O.O=, par Pizzacakecomic , type: Reddit\tNombre de commentaires : 509\n",
      "?, par BagMean3089 , type: Reddit\tNombre de commentaires : 173\n",
      "[Wojnarowski] Pacers star Tyrese Haliburton has a Grade 1 left hamstring strain, an MRI revealed on Tuesday, sources tell ESPN. He’s expected to be re-evaluated in approximately two weeks but there’s relief that he’s avoided serious injury., par MarvelsGrantMan136 , type: Reddit\tNombre de commentaires : 237\n",
      "A Bayesian View of the Poisson-Dirichlet Process, par Wray Buntine , type: Arxiv\tCo-auteurs : Marcus Hutter\n",
      "A consequence of pressure difference, par oklolzzzzs , type: Reddit\tNombre de commentaires : 412\n",
      "A different approach , par Indieriots , type: Reddit\tNombre de commentaires : 168\n",
      "A Hierarchical Dirichlet Process Model with Multiple Levels of  Clustering for Human EEG Seizure Modeling, par Drausin Wulsin , type: Arxiv\tCo-auteurs : Shane Jensen, Brian Litt\n",
      "A normal western movie, par disgustinghonnor , type: Reddit\tNombre de commentaires : 250\n",
      "A very interesting and heartwarming concept, par Tex-the-Dragon , type: Reddit\tNombre de commentaires : 677\n",
      "A very tough decision, par L3veLUP , type: Reddit\tNombre de commentaires : 296\n",
      "Absolutely disturbing but never surprising unfortunately., par The_Kyojuro_Rengoku , type: Reddit\tNombre de commentaires : 277\n",
      "Aged HORRENDOUSLY, par Lord_Answer_me_Why , type: Reddit\tNombre de commentaires : 69\n",
      "An efficient algorithm for solving elliptic problems on percolation  clusters, par Chenlin Gu , type: Arxiv\n",
      "Asking the boys to craft the perfect text to her, par Few_Independence7489 , type: Reddit\tNombre de commentaires : 280\n",
      "Babe wake up, new dream job just dropped, par elvis-wantacookie , type: Reddit\tNombre de commentaires : 152\n",
      "backToStackOverflowAgain, par FelchingLegend , type: Reddit\tNombre de commentaires : 279\n",
      "Bayesian Clustering of Transcription Factor Binding Motifs, par Shane T. Jensen , type: Arxiv\tCo-auteurs : Jun S. Liu\n",
      "Bayesian mixture models (in)consistency for the number of clusters, par Louise Alamichel , type: Arxiv\tCo-auteurs : Daria Bystrova, Julyan Arbel, Guillaume Kon Kam King\n",
      "Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts, par Vu Nguyen , type: Arxiv\tCo-auteurs : Dinh Phung, XuanLong Nguyen, Svetha Venkatesh, Hung Hai Bui\n",
      "Bayesian nonparametric temporal dynamic clustering via autoregressive  Dirichlet priors, par Maria De Iorio , type: Arxiv\tCo-auteurs : Stefano Favaro, Alessandra Guglielmi, Lifeng Ye\n",
      "BBC apologizes for running Hamas claim IDF executed Gaza civilians, par Dvbrch , type: Reddit\tNombre de commentaires : 549\n",
      "Beta-Product Poisson-Dirichlet Processes, par Federico Bassetti , type: Arxiv\tCo-auteurs : Roberto Casarin, Fabrizio Leisen\n",
      "Bi_irl, par BenevolentDinosaur , type: Reddit\tNombre de commentaires : 130\n",
      "Birtherism comes for Nikki Haley, par HeHateMe337 , type: Reddit\tNombre de commentaires : 439\n",
      "Boy comforts puppy scared of bath time, par JovialJules , type: Reddit\tNombre de commentaires : 141\n",
      "Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with  Graphs for Passenger Trajectory Clustering, par Ziyue Li , type: Arxiv\tCo-auteurs : Hao Yan, Chen Zhang, Lijun Sun, Wolfgang Ketter, Fugee Tsung\n",
      "Christ, what an asshole, par knivesofsmoothness , type: Reddit\tNombre de commentaires : 666\n",
      "Clustering consistency with Dirichlet process mixtures, par Filippo Ascolani , type: Arxiv\tCo-auteurs : Antonio Lijoi, Giovanni Rebaudo, Giacomo Zanella\n",
      "Colouring and breaking sticks: random distributions and heterogeneous  clustering, par Peter J. Green , type: Arxiv\n",
      "Conjoined Dirichlet Process, par Michelle N. Ngo , type: Arxiv\tCo-auteurs : Dustin S. Pluta, Alexander N. Ngo, Babak Shahbaba\n",
      "Consistency Analysis for the Doubly Stochastic Dirichlet Process, par Xing Sun , type: Arxiv\tCo-auteurs : Nelson H. C. Yung, Edmund Y. Lam, Hayden K. -H. So\n",
      "Cursed_hamster, par wedaiebaldank , type: Reddit\tNombre de commentaires : 105\n",
      "Dafuq the bunny doinn'?!, par Voodoo_Bobcat , type: Reddit\tNombre de commentaires : 111\n",
      "Damn Athenians, you ruined Athens!, par mcflymikes , type: Reddit\tNombre de commentaires : 33\n",
      "DIMM-SC: A Dirichlet mixture model for clustering droplet-based single  cell transcriptomic data, par Zhe Sun , type: Arxiv\tCo-auteurs : Ting Wang, Ke Deng, Xiao-Feng Wang, Robert Lafyatis, Ying Ding, Ming Hu, Wei Chen\n",
      "Dirichlet Fragmentation Processes, par Hong Ge , type: Arxiv\tCo-auteurs : Yarin Gal, Zoubin Ghahramani\n",
      "Dirichlet Process Mixtures of Generalized Mallows Models, par Marina Meila , type: Arxiv\tCo-auteurs : Harr Chen\n",
      "Dirichlet-tree multinomial mixtures for clustering microbiome  compositions, par Jialiang Mao , type: Arxiv\tCo-auteurs : Li Ma\n",
      "Disabled Cellist Gives Moving Performance, par RevolutionaryTell668 , type: Reddit\tNombre de commentaires : 129\n",
      "DIVA: A Dirichlet Process Mixtures Based Incremental Deep Clustering  Algorithm via Variational Auto-Encoder, par Zhenshan Bing , type: Arxiv\tCo-auteurs : Yuan Meng, Yuqi Yun, Hang Su, Xiaojie Su, Kai Huang, Alois Knoll\n",
      "Dope with a knife, par WolfGodlives , type: Reddit\tNombre de commentaires : 361\n",
      "Driving the first ever car from 1886 that was built by Mercedes-Benz, par Far-Stay9417 , type: Reddit\tNombre de commentaires : 528\n",
      "Dude went in a 1v2 and still won, par Ok-Arm8050 , type: Reddit\tNombre de commentaires : 265\n",
      "Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process  Mixture, par Trevor Campbell , type: Arxiv\tCo-auteurs : Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin\n",
      "Erm… Hello??, par OggoChoggo , type: Reddit\tNombre de commentaires : 260\n",
      "Fair Clustering via Hierarchical Fair-Dirichlet Process, par Abhisek Chakraborty , type: Arxiv\tCo-auteurs : Anirban Bhattacharya, Debdeep Pati\n",
      "Family Reunion, par DragonDuChat , type: Reddit\tNombre de commentaires : 289\n",
      "Flexible clustering via hidden hierarchical Dirichlet priors, par Antonio Lijoi , type: Arxiv\tCo-auteurs : Igor Prünster, Giovanni Rebaudo\n",
      "Flexible Priors for Exemplar-based Clustering, par Daniel Tarlow , type: Arxiv\tCo-auteurs : Richard S. Zemel, Brendan J. Frey\n",
      "France just designated it's first openly gay government leader (Prime Minister), par NeimaDParis , type: Reddit\tNombre de commentaires : 752\n",
      "From here to infinity - sparse finite versus Dirichlet process mixtures  in model-based clustering, par Sylvia Frühwirth-Schnatter , type: Arxiv\tCo-auteurs : Gertraud Malsiner-Walli\n",
      "Functional clustering in nested designs: Modeling variability in  reproductive epidemiology studies, par Abel Rodriguez , type: Arxiv\tCo-auteurs : David B. Dunson\n",
      "Gibbs Sampling for (Coupled) Infinite Mixture Models in the Stick  Breaking Representation, par Ian Porteous , type: Arxiv\tCo-auteurs : Alexander T. Ihler, Padhraic Smyth, Max Welling\n",
      "GOP frontrunner openly advocates for crashing the economy, par 8-bit-Felix , type: Reddit\tNombre de commentaires : 223\n",
      "Graphical Dirichlet Process for Clustering Non-Exchangeable Grouped Data, par Arhit Chakrabarti , type: Arxiv\tCo-auteurs : Yang Ni, Ellen Ruth A. Morris, Michael L. Salinas, Robert S. Chapkin, Bani K. Mallick\n",
      "Guard rail saves the day, par Joey_Devoe , type: Reddit\tNombre de commentaires : 182\n",
      "He thinks I can’t see him, par No-Specific1660 , type: Reddit\tNombre de commentaires : 169\n",
      "He thought he was unlovable. Little did he know, that this video would lead him to meeting the love of his life., par New-Corgi-1685 , type: Reddit\tNombre de commentaires : 284\n",
      "Here's a reason to live longer, par BayPangoro , type: Reddit\tNombre de commentaires : 159\n",
      "Heterogeneous Regression Models for Clusters of Spatial Dependent Data, par Zhihua Ma , type: Arxiv\tCo-auteurs : Yishu Xue, Guanyu Hu\n",
      "Hierarchical Dirichlet Process and Relative Entropy, par Shui Feng , type: Arxiv\n",
      "Hierarchical Latent Word Clustering, par Halid Ziya Yerebakan , type: Arxiv\tCo-auteurs : Fitsum Reda, Yiqiang Zhan, Yoshihisa Shinagawa\n",
      "How would you respond, par honeyyzzzx , type: Reddit\tNombre de commentaires : 131\n",
      "I got my car back from the shop six weeks ago, and only just realized..., par Straight-Dish-7074 , type: Reddit\tNombre de commentaires : 208\n",
      "I mean, little bit, par OmegaBoi420 , type: Reddit\tNombre de commentaires : 65\n",
      "I'm the only student in my online Fundamentals of Accounting class at City University of Seattle., par mydude356 , type: Reddit\tNombre de commentaires : 120\n",
      "Ich_iel, par abradolf__lincler_ , type: Reddit\tNombre de commentaires : 178\n",
      "If Skyrim was made in the 90s, par hozzam11 , type: Reddit\tNombre de commentaires : 263\n",
      "Indeed!, par Justthisdudeyaknow , type: Reddit\tNombre de commentaires : 589\n",
      "Informed Bayesian Finite Mixture Models via Asymmetric Dirichlet Priors, par Garritt L. Page , type: Arxiv\tCo-auteurs : Massimo Ventrucci, Maria Franco-Villoria\n",
      "Invariant Percolation and Harmonic Dirichlet Functions, par Damien Gaboriau , type: Arxiv\n",
      "It's curious to think about One Piece timeline, par Ramondasetemeia , type: Reddit\tNombre de commentaires : 69\n",
      "Jimmy Kimmel's monologue response tonight to Aaron Rodgers falsely accusing him of being on the Jeffrey Epstein list, par SappyGilmore , type: Reddit\tNombre de commentaires : 1551\n",
      "Joint Clustering and Registration of Functional Data, par Yafeng Zhang , type: Arxiv\tCo-auteurs : Donatello Telesca\n",
      "Jon Favreau Set To Direct New 'Star Wars' Movie 'The Mandalorian & Grogu', Begins Production This Year, par MarvelsGrantMan136 , type: Reddit\tNombre de commentaires : 761\n",
      "Knife defense techniques for noobs (that probably don't work), par SixxRabbit , type: Reddit\tNombre de commentaires : 319\n",
      "Love has no boundaries (virgin edition), par Ordinary-Bluebird979 , type: Reddit\tNombre de commentaires : 394\n",
      "Me and my sister 2004/2005, par m-e-s-o , type: Reddit\tNombre de commentaires : 795\n",
      "Me irl, par TheWebsploiter , type: Reddit\tNombre de commentaires : 961\n",
      "me_irl, par prentisswife , type: Reddit\tNombre de commentaires : 19\n",
      "Meirl, par Dramatic_Carpet_6589 , type: Reddit\tNombre de commentaires : 276\n",
      "Meirl, par -kyutiepie- , type: Reddit\tNombre de commentaires : 26\n",
      "Mortuary Assistant Scares YouTuber, par Mildred_Wolfenbarger , type: Reddit\tNombre de commentaires : 190\n",
      "My friends and I made an infographics with the best games of 2023 ranged by Metacritic, par brontozawr , type: Reddit\tNombre de commentaires : 937\n",
      "My grey car is turning orange???, par Raiceboi , type: Reddit\tNombre de commentaires : 804\n",
      "My mom and dad’s first date, 1992, par glitterycroissant , type: Reddit\tNombre de commentaires : 239\n",
      "My whole life was a lie!, par Upstairs-Story-8661 , type: Reddit\tNombre de commentaires : 86\n",
      "Nightmare on a night out: Woman claims on TikTok that she accidentally ordered a $2,400 cocktail, par Outside_Mongoose_749 , type: Reddit\tNombre de commentaires : 368\n",
      "Nonparametric Variable Selection, Clustering and Prediction for  High-Dimensional Regression, par Subharup Guha , type: Arxiv\tCo-auteurs : Veerabhadran Baladandayuthapani\n",
      "Numerical conformal mapping with rational functions, par Lloyd N. Trefethen , type: Arxiv\n",
      "Nurse was ignoring my pain in hospital today, so I asked my husband to call and ask her what the deal is. He then came in and she totally changed her tune, par Electromagneticpoms , type: Reddit\tNombre de commentaires : 173\n",
      "On the Variational Posterior of Dirichlet Process Deep Latent Gaussian  Mixture Models, par Amine Echraibi , type: Arxiv\tCo-auteurs : Joachim Flocon-Cholet, Stéphane Gosselin, Sandrine Vaton\n",
      "oopsie poopsie, par constantlytired1917 , type: Reddit\tNombre de commentaires : 105\n",
      "Parallel Clustering of Single Cell Transcriptomic Data with Split-Merge  Sampling on Dirichlet Process Mixtures, par Tiehang Duan , type: Arxiv\tCo-auteurs : José P. Pinto, Xiaohui Xie\n",
      "Percolation Perturbations in Potential Theory and Random Walks, par Itai Benjamini , type: Arxiv\tCo-auteurs : Russell Lyons, Oded Schramm\n",
      "Posterior Distribution for the Number of Clusters in Dirichlet Process  Mixture Models, par Chiao-Yu Yang , type: Arxiv\tCo-auteurs : Eric Xia, Nhat Ho, Michael I. Jordan\n",
      "Powered Dirichlet Process for Controlling the Importance of  \"Rich-Get-Richer\" Prior Assumptions in Bayesian Clustering, par Gaël Poux-Médard , type: Arxiv\tCo-auteurs : Julien Velcin, Sabine Loudcher\n",
      "Powered Hawkes-Dirichlet Process: Challenging Textual Clustering using a  Flexible Temporal Prior, par Gaël Poux-Médard , type: Arxiv\tCo-auteurs : Julien Velcin, Sabine Loudcher\n",
      "Product Centered Dirichlet Processes for Dependent Clustering, par Alexander Dombowsky , type: Arxiv\tCo-auteurs : David B. Dunson\n",
      "Puritanical Feelings > Reality, par TruCynic , type: Reddit\tNombre de commentaires : 178\n",
      "Really brave, par WorldlinessOk7796 , type: Reddit\tNombre de commentaires : 332\n",
      "Regret [OC], par MelonKony , type: Reddit\tNombre de commentaires : 132\n",
      "Revisiting k-means: New Algorithms via Bayesian Nonparametrics, par Brian Kulis , type: Arxiv\tCo-auteurs : Michael I. Jordan\n",
      "Roger Stone reportedly said leading Democratic congressman ‘has to die’, par nosotros_road_sodium , type: Reddit\tNombre de commentaires : 342\n",
      "Samsung introduces the world's first transparent MicroLED screen at CES 2024, par oblique_shockwave , type: Reddit\tNombre de commentaires : 1585\n",
      "Scalable Inference for Latent Dirichlet Allocation, par James Petterson , type: Arxiv\tCo-auteurs : Tiberio Caetano\n",
      "Second update- AITA for forcing my daughter to share a room, par Fine-Neat3967 , type: Reddit\tNombre de commentaires : 336\n",
      "Similarity-based Random Partition Distribution for Clustering Functional  Data, par Tomoya Wakayama , type: Arxiv\tCo-auteurs : Shonosuke Sugasawa, Genya Kobayashi\n",
      "Sinead O'Connor died of natural causes, par Odd_Responsibility_5 , type: Reddit\tNombre de commentaires : 435\n",
      "Smacked by his own platform, par GleamingBound , type: Reddit\tNombre de commentaires : 745\n",
      "Soon to be my Mother-in-law, par 420Litten , type: Reddit\tNombre de commentaires : 441\n",
      "Spectral analysis of communication networks using Dirichlet eigenvalues, par Alexander Tsiatas , type: Arxiv\tCo-auteurs : Iraj Saniee, Onuttom Narayan, Matthew Andrews\n",
      "Start a business with Colombian cheese. Good luck., par Usual_Key_9726 , type: Reddit\tNombre de commentaires : 73\n",
      "Tensor Dirichlet Process Multinomial Mixture Model for Passenger  Trajectory Clustering, par Ziyue Li , type: Arxiv\tCo-auteurs : Hao Yan, Chen Zhang, Andi Wang, Wolfgang Ketter, Lijun Sun, Fugee Tsung\n",
      "Thanks for bringing this to our attention, Donny, Jr., par jared10011980 , type: Reddit\tNombre de commentaires : 1232\n",
      "That mug, par NBL_123 , type: Reddit\tNombre de commentaires : 669\n",
      "The Hunger Games, par Super_Emu_7291 , type: Reddit\tNombre de commentaires : 43\n",
      "The more you look, the more you see, par Powerfluffgirl20 , type: Reddit\tNombre de commentaires : 132\n",
      "The semi-hierarchical Dirichlet Process and its application to  clustering homogeneous distributions, par Mario Beraha , type: Arxiv\tCo-auteurs : Alessandra Guglielmi, Fernando A. Quintana\n",
      "The supervised hierarchical Dirichlet process, par Andrew M. Dai , type: Arxiv\tCo-auteurs : Amos J. Storkey\n",
      "The Teen TV Starter Pack, par palmerry , type: Reddit\tNombre de commentaires : 47\n",
      "The wage theft crime wave strikes again, par GrandpaChainz , type: Reddit\tNombre de commentaires : 71\n",
      "The way this label saves space to write 'energy' in many languages, par Eigenurin , type: Reddit\tNombre de commentaires : 359\n",
      "These three regions make up 50% of world GDP, par redditor3000 , type: Reddit\tNombre de commentaires : 578\n",
      "This hilarious mural at a taco shop, par howsyourdayoffamigo , type: Reddit\tNombre de commentaires : 96\n",
      "This is the Republican's idea of \"winning.\" They all need to be voted out of office, par rhino910 , type: Reddit\tNombre de commentaires : 78\n",
      "This subreddit vs the rest of the world, par ThiccAssCrackHead , type: Reddit\tNombre de commentaires : 321\n",
      "TIL Boeing pressured the US government to impose a 300% tariff on imports of Bombardier CSeries planes. The situation got bad enough that Canada filed a complaint at the WTO against the US. Eventually, Bombardier subsequently sold a 50.01% in the plane to Boeing's main competitor, Airbus, for $1., par PretendAsparaguso , type: Reddit\tNombre de commentaires : 893\n",
      "TIL Korean astronauts eat a version of kimchi that has been radiated to kill all of the microorganisms, par admiralturtleship , type: Reddit\tNombre de commentaires : 124\n",
      "to target a commercial airplane , par HornyDiggler , type: Reddit\tNombre de commentaires : 422\n",
      "Topic Detection from Conversational Dialogue Corpus with Parallel  Dirichlet Allocation Model and Elbow Method, par Haider Khalid , type: Arxiv\tCo-auteurs : Vincent Wade\n",
      "Training blindfolded, par baddiebee22give , type: Reddit\tNombre de commentaires : 198\n",
      "Turntable, par Daddys_Girlo , type: Reddit\tNombre de commentaires : 306\n",
      "UNBGBBIIVCHIDCTIICBG&Weeeeeeeeeeeeeeeeeee, par 2tooORtutu , type: Reddit\tNombre de commentaires : 84\n",
      "Unsupervised Outlier Detection using Random Subspace and Subsampling  Ensembles of Dirichlet Process Mixtures, par Dongwook Kim , type: Arxiv\tCo-auteurs : Juyeon Park, Hee Cheol Chung, Seonghyun Jeong\n",
      "UPDATE: turns out homeboy never had HIV. Ex gf made it all up, par Marcellius-the-3rd , type: Reddit\tNombre de commentaires : 407\n",
      "Use your nice stuff, par mando0987654321 , type: Reddit\tNombre de commentaires : 85\n",
      "Watch: GOP Official Panics When Asked for Reason to Remove Biden From Ballot, par thenewrepublic , type: Reddit\tNombre de commentaires : 801\n",
      "What everyday item do you think will become obsolete in the next 10 years, and why?, par Electrical-Memory-81 , type: Reddit\tNombre de commentaires : 3881\n",
      "What's going on at your workplace?, par Arijnkoju7467 , type: Reddit\tNombre de commentaires : 58\n",
      "What's your opinion?, par Competitive_Office68 , type: Reddit\tNombre de commentaires : 225\n",
      "Where there’s a kitty there’s a way., par Umer_- , type: Reddit\tNombre de commentaires : 43\n",
      "Why am I not rich yet?, par lovelyxxy , type: Reddit\tNombre de commentaires : 23\n",
      "Willem Dafoe receives star on Hollywood Walk of Fame, par unknown_human , type: Reddit\tNombre de commentaires : 409\n",
      "Would you still get married?, par Ordinary_Process_920 , type: Reddit\tNombre de commentaires : 170\n",
      "X Purges Prominent Journalists, Leftists With No Explanation, par mcmeaningoflife42 , type: Reddit\tNombre de commentaires : 2133\n",
      "you are the reason they stay home., par diamondxxxl , type: Reddit\tNombre de commentaires : 171\n",
      "🔥 Speed of the hunt , par satishtreks , type: Reddit\tNombre de commentaires : 218\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(\"Mon corpus\")\n",
    "\n",
    "for doc in collection:\n",
    "    corpus.add(doc)\n",
    "#corpus.show(tri=\"abc\")\n",
    "#print(repr(corpus))\n",
    "\n",
    "# Sauvegarde\n",
    "# Vérifier si le fichier 'corpus.csv' existe déjà\n",
    "csv_file_exists = path.exists('corpus.csv')\n",
    "pickle_file_exists = path.exists('corpus.pkl')\n",
    "\n",
    "if not pickle_file_exists: \n",
    "    import pickle\n",
    "\n",
    "    # Ouverture d'un fichier, puis écriture avec pickle\n",
    "    with open(\"corpus.pkl\", \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "    # Supression de la variable \"corpus\"\n",
    "    del corpus\n",
    "\n",
    "    # Ouverture du fichier, puis lecture avec pickle\n",
    "    with open(\"corpus.pkl\", \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "\n",
    "    # La variable est réapparue\n",
    "    print(\"Corpus brut\", corpus)\n",
    "\n",
    "if not csv_file_exists:\n",
    "    # Création du contenu CSV\n",
    "    nature=corpus.elements_du_corpus()[0]\n",
    "    titre=corpus.elements_du_corpus()[1]\n",
    "    Auteur=corpus.elements_du_corpus()[2]\n",
    "    Co_Auteurs=corpus.elements_du_corpus()[3]\n",
    "    nb_commentaires=corpus.elements_du_corpus()[4]\n",
    "    Date=corpus.elements_du_corpus()[5]\n",
    "    url=corpus.elements_du_corpus()[6]\n",
    "    texte=corpus.elements_du_corpus()[7]\n",
    "    \n",
    "    # Transformation des données en csv\n",
    "    df = pd.DataFrame(zip(nature,titre,Auteur,Co_Auteurs,Date,url,texte,nb_commentaires), columns=['Nature','Titre','Auteur','Co_Auteurs','Date','URL','Texte','Nb commentaires'])\n",
    "    df.to_csv(r'corpus.csv',index=False,sep=';')\n",
    "    \n",
    "    print(\"Creation du CSV OK\")\n",
    "\n",
    "else:\n",
    "    print(\"Le corpus et le fichier CSV existent déjà.\")\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f9e37-b650-4576-9b87-f7fb92c6eef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
