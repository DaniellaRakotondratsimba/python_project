{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805649d0-d648-404f-8e5d-0681d5f0ad04",
   "metadata": {},
   "source": [
    "# Importer les librairies et les classes nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abef9211-d7e9-4e5b-aeef-fbbee960b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import urllib, urllib.request\n",
    "import xmltodict\n",
    "import datetime\n",
    "import pickle\n",
    "from Classes import Document, RedditDocument, ArxivDocument\n",
    "from Classes import Document\n",
    "from Classes import Author\n",
    "from Corpus import Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7984ab-9197-49ab-bd40-4d61127c477e",
   "metadata": {},
   "source": [
    "# Fonction pour afficher la structure hiérarchique d'un dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c06368-8ae0-4a69-b5f0-15097c1af928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showDictStruct(d):\n",
    "    def recursivePrint(d, i):\n",
    "        for k in d:\n",
    "            if isinstance(d[k], dict):\n",
    "                print(\"-\"*i, k)\n",
    "                recursivePrint(d[k], i+2)\n",
    "            else:\n",
    "                print(\"-\"*i, k, \":\", d[k])\n",
    "    recursivePrint(d, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7615a-fd78-4714-b206-3178b4329313",
   "metadata": {},
   "source": [
    "# Configuration de l'API Reddit à l'aide la bibliothèque praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c688f1d-22bd-4c9c-9a5d-0a388a72894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='wfrwVmRrikEa9yhkXWMzMQ', client_secret='ahqeIx-Xp4pbFgGccGsLrrd1aecWkg', user_agent='projet_python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c5f12-3f73-4355-8707-b6bf97363911",
   "metadata": {},
   "source": [
    "## Récupération des 100 publications populaires sur Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58386a52-be37-407d-b59d-03a8ebb3b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100\n",
    "hot_posts = reddit.subreddit('all').hot(limit=limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc081e2-69e7-4c13-98db-db23ed2100a5",
   "metadata": {},
   "source": [
    "### Récupération des données à partir de ces 100 publications populaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d74d2f3-cc17-4f89-8b81-e8587f64fb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "docs_bruts = []\n",
    "afficher_cles = False\n",
    "for i, post in enumerate(hot_posts):\n",
    "    if i%10==0: print(\"Reddit:\", i, \"/\", limit)\n",
    "    # Pour connaître les différentes variables et leur contenu\n",
    "    if afficher_cles:  \n",
    "        for k, v in post.__dict__.items():\n",
    "            pass\n",
    "            print(k, \":\", v)\n",
    "\n",
    "    # On ne considère pas les posts sans texte\n",
    "    if post.selftext != \"\":  \n",
    "        pass\n",
    "    docs.append(post.selftext.replace(\"\\n\", \" \"))\n",
    "    docs_bruts.append((\"Reddit\", post))\n",
    "\n",
    "print (docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b63d4-59c8-4cbe-bd4a-871325e899af",
   "metadata": {},
   "source": [
    "# Récupération des résumés de publications ArXiv avec une requête API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3d31c0-b150-4fe2-94c5-426f9b815f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArXiv: 0 / 100\n",
      "ArXiv: 10 / 100\n",
      "ArXiv: 20 / 100\n",
      "ArXiv: 30 / 100\n",
      "ArXiv: 40 / 100\n"
     ]
    }
   ],
   "source": [
    "# Paramètres\n",
    "query_terms = [\"clustering\", \"Dirichlet\"]\n",
    "max_results = 50\n",
    "\n",
    "# Requête\n",
    "url = f'http://export.arxiv.org/api/query?search_query=all:{\"+\".join(query_terms)}&start=0&max_results={max_results}'\n",
    "data = urllib.request.urlopen(url)\n",
    "\n",
    "# Format dict (OrderedDict)\n",
    "data = xmltodict.parse(data.read().decode('utf-8'))\n",
    "\n",
    "#showDictStruct(data)\n",
    "\n",
    "# Ajout résumés à la liste\n",
    "for i, entry in enumerate(data[\"feed\"][\"entry\"]):\n",
    "    if i%10==0: print(\"ArXiv:\", i, \"/\", limit)\n",
    "    docs.append(entry[\"summary\"].replace(\"\\n\", \"\"))\n",
    "    docs_bruts.append((\"ArXiv\", entry))\n",
    "    #showDictStruct(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10c626-1bee-43cd-adbb-00d65c6a1956",
   "metadata": {},
   "source": [
    "# Prétraitement et exploitation des données de Reddit et Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01b78a6-54b0-4082-854b-c93e1bf78891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# docs avec doublons : 50\n",
      "# docs sans doublons : 50\n",
      "Document 0\t# caractères : 1096\t# mots : 141\t# phrases : 8\n",
      "Document 1\t# caractères : 1023\t# mots : 132\t# phrases : 6\n",
      "Document 2\t# caractères : 1297\t# mots : 166\t# phrases : 12\n",
      "Document 3\t# caractères : 1360\t# mots : 176\t# phrases : 10\n",
      "Document 4\t# caractères : 394\t# mots : 55\t# phrases : 8\n",
      "Document 5\t# caractères : 513\t# mots : 63\t# phrases : 5\n",
      "Document 6\t# caractères : 1715\t# mots : 214\t# phrases : 9\n",
      "Document 7\t# caractères : 1012\t# mots : 146\t# phrases : 7\n",
      "Document 8\t# caractères : 950\t# mots : 133\t# phrases : 7\n",
      "Document 9\t# caractères : 1006\t# mots : 134\t# phrases : 7\n",
      "Document 10\t# caractères : 1096\t# mots : 157\t# phrases : 7\n",
      "Document 11\t# caractères : 756\t# mots : 100\t# phrases : 8\n",
      "Document 12\t# caractères : 969\t# mots : 124\t# phrases : 6\n",
      "Document 13\t# caractères : 1831\t# mots : 211\t# phrases : 12\n",
      "Document 14\t# caractères : 909\t# mots : 125\t# phrases : 6\n",
      "Document 15\t# caractères : 1876\t# mots : 257\t# phrases : 11\n",
      "Document 16\t# caractères : 423\t# mots : 56\t# phrases : 4\n",
      "Document 17\t# caractères : 1196\t# mots : 158\t# phrases : 7\n",
      "Document 18\t# caractères : 627\t# mots : 84\t# phrases : 7\n",
      "Document 19\t# caractères : 1360\t# mots : 180\t# phrases : 7\n",
      "Document 20\t# caractères : 828\t# mots : 112\t# phrases : 6\n",
      "Document 21\t# caractères : 1247\t# mots : 149\t# phrases : 8\n",
      "Document 22\t# caractères : 670\t# mots : 82\t# phrases : 5\n",
      "Document 23\t# caractères : 984\t# mots : 121\t# phrases : 7\n",
      "Document 24\t# caractères : 1013\t# mots : 133\t# phrases : 9\n",
      "Document 25\t# caractères : 1320\t# mots : 167\t# phrases : 8\n",
      "Document 26\t# caractères : 1315\t# mots : 197\t# phrases : 9\n",
      "Document 27\t# caractères : 775\t# mots : 103\t# phrases : 6\n",
      "Document 28\t# caractères : 802\t# mots : 107\t# phrases : 7\n",
      "Document 29\t# caractères : 997\t# mots : 148\t# phrases : 9\n",
      "Document 30\t# caractères : 1347\t# mots : 169\t# phrases : 11\n",
      "Document 31\t# caractères : 1600\t# mots : 218\t# phrases : 11\n",
      "Document 32\t# caractères : 1029\t# mots : 136\t# phrases : 7\n",
      "Document 33\t# caractères : 1894\t# mots : 246\t# phrases : 10\n",
      "Document 34\t# caractères : 1033\t# mots : 129\t# phrases : 8\n",
      "Document 35\t# caractères : 1137\t# mots : 165\t# phrases : 7\n",
      "Document 36\t# caractères : 949\t# mots : 125\t# phrases : 6\n",
      "Document 37\t# caractères : 1201\t# mots : 162\t# phrases : 14\n",
      "Document 38\t# caractères : 1100\t# mots : 146\t# phrases : 7\n",
      "Document 39\t# caractères : 977\t# mots : 137\t# phrases : 8\n",
      "Document 40\t# caractères : 423\t# mots : 54\t# phrases : 4\n",
      "Document 41\t# caractères : 833\t# mots : 117\t# phrases : 9\n",
      "Document 42\t# caractères : 950\t# mots : 129\t# phrases : 8\n",
      "Document 43\t# caractères : 1155\t# mots : 150\t# phrases : 8\n",
      "Document 44\t# caractères : 1043\t# mots : 156\t# phrases : 9\n",
      "Document 45\t# caractères : 1817\t# mots : 246\t# phrases : 11\n",
      "Document 46\t# caractères : 418\t# mots : 63\t# phrases : 4\n",
      "Document 47\t# caractères : 609\t# mots : 86\t# phrases : 5\n",
      "Document 48\t# caractères : 724\t# mots : 94\t# phrases : 4\n",
      "Document 49\t# caractères : 1261\t# mots : 172\t# phrases : 11\n"
     ]
    }
   ],
   "source": [
    "# Affichage et suppression des doublons\n",
    "print(f\"# docs avec doublons : {len(docs)}\")\n",
    "docs = list(set(docs))\n",
    "print(f\"# docs sans doublons : {len(docs)}\")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i}\\t# caractères : {len(doc)}\\t# mots : {len(doc.split(' '))}\\t# phrases : {len(doc.split('.'))}\")\n",
    "    if len(doc)<100:\n",
    "        docs.remove(doc)\n",
    "\n",
    "# Concaténation des documents restants\n",
    "longueChaineDeCaracteres = \" \".join(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8643f90-fd71-420b-99f4-ceb34716ba65",
   "metadata": {},
   "source": [
    "# Factory Pattern pour la création d'instances de documents à partir de données brutes\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e022677-43e8-4feb-b39c-ee318c9baa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = []\n",
    "# FACTORY PATTERN\n",
    "class DocumentFactory:\n",
    "    @staticmethod\n",
    "    def create_document(nature, doc):\n",
    "        if nature == \"ArXiv\":  # Les fichiers de ArXiv ou de Reddit ne sont pas formatés de la même manière à ce stade.\n",
    "        #showDictStruct(doc)\n",
    "\n",
    "            titre = doc[\"title\"].replace('\\n', '')  # On enlève les retours à la ligne\n",
    "            \"\"\"try:\n",
    "                authors = \", \".join([a[\"name\"] for a in doc[\"author\"]])  # On fait une liste d'auteurs, séparés par une virgule\n",
    "            except:\n",
    "                authors = doc[\"author\"][\"name\"]  # Si l'auteur est seul, pas besoin de liste\"\"\"\n",
    "            # récupérer l'auteur et les co-auteur\n",
    "            try:\n",
    "                authors_list = [a[\"name\"] for a in doc[\"author\"]]\n",
    "                primary_author = authors_list[0]\n",
    "                co_authors = authors_list[1:] if len(authors_list) > 1 else []\n",
    "            except TypeError:\n",
    "                primary_author = doc[\"author\"][\"name\"]\n",
    "                co_authors = []\n",
    "            summary = doc[\"summary\"].replace(\"\\n\", \"\")  # On enlève les retours à la ligne\n",
    "            date = datetime.datetime.strptime(doc[\"published\"], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y/%m/%d\")  # Formatage de la date en année/mois/jour avec librairie datetime\n",
    "\n",
    "            #return ArxivDocument(titre, authors, date, doc[\"id\"], summary) \n",
    "            return ArxivDocument(titre, primary_author, date, doc[\"id\"], summary, co_auteurs=co_authors)# Création du Document\n",
    "           \n",
    "        elif nature == \"Reddit\":\n",
    "            titre = doc.title.replace(\"\\n\", '')\n",
    "            auteur = str(doc.author)\n",
    "            date = datetime.datetime.fromtimestamp(doc.created).strftime(\"%Y/%m/%d\")\n",
    "            url = \"https://www.reddit.com/\"+doc.permalink\n",
    "            texte = doc.selftext.replace(\"\\n\", \"\")\n",
    "            nb_commentaires = doc.num_comments\n",
    "            return RedditDocument(titre, auteur, date, url, texte, nb_commentaires)\n",
    "        else:\n",
    "            raise ValueError(\"Nature de document non supportée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee8bff-0988-46a0-832c-717c147c5111",
   "metadata": {},
   "source": [
    "### Utilisation du factory pour créer des objets Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26537d43-87c7-4fba-8c32-d950c2a5f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nature, doc in docs_bruts:\n",
    "    doc_classe = DocumentFactory.create_document(nature, doc)\n",
    "    collection.append(doc_classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb3502-1fb7-407e-850e-379fc1c57e39",
   "metadata": {},
   "source": [
    "### Création de l'index de documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca4c67df-c626-411b-a04c-6f0b4581d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2doc = {}\n",
    "for i, doc in enumerate(collection):\n",
    "    id2doc[i] = doc.titre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec7b56-3c97-4255-aaa2-fe85612115be",
   "metadata": {},
   "source": [
    "### Gestion des Auteurs et création d'une liste d'objets Auteur avec index associé\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "255f6e07-4a28-4891-a932-ca80890a5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = {}\n",
    "aut2id = {}\n",
    "num_auteurs_vus = 0\n",
    "\n",
    "# Création de la liste et l'index des Auteurs\n",
    "for doc in collection:\n",
    "    if doc.auteur not in aut2id:\n",
    "        num_auteurs_vus += 1\n",
    "        authors[num_auteurs_vus] = Author(doc.auteur)\n",
    "        aut2id[doc.auteur] = num_auteurs_vus\n",
    "\n",
    "    authors[aut2id[doc.auteur]].add(doc.texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3afda45-fbe5-435e-ac99-0f8689cbea52",
   "metadata": {},
   "source": [
    "# Construction du corpus à partir des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e467b590-6292-404a-a816-69729eaff019",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m     corpus\u001b[38;5;241m.\u001b[39madd(doc)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#corpus.show(tri=\"abc\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#print(repr(corpus))\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Sauvegarde\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Vérifier si le fichier 'corpus.csv' existe déjà\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m csv_file_exists \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpus.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m pickle_file_exists \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorpus.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pickle_file_exists: \n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(\"Mon corpus\")\n",
    "\n",
    "for doc in collection:\n",
    "    corpus.add(doc)\n",
    "#corpus.show(tri=\"abc\")\n",
    "#print(repr(corpus))\n",
    "\n",
    "# Sauvegarde\n",
    "# Vérifier si le fichier 'corpus.csv' existe déjà\n",
    "csv_file_exists = path.exists('corpus.csv')\n",
    "pickle_file_exists = path.exists('corpus.pkl')\n",
    "\n",
    "if not pickle_file_exists: \n",
    "    import pickle\n",
    "\n",
    "    # Ouverture d'un fichier, puis écriture avec pickle\n",
    "    with open(\"corpus.pkl\", \"wb\") as f:\n",
    "        pickle.dump(corpus, f)\n",
    "\n",
    "    # Supression de la variable \"corpus\"\n",
    "    del corpus\n",
    "\n",
    "    # Ouverture du fichier, puis lecture avec pickle\n",
    "    with open(\"corpus.pkl\", \"rb\") as f:\n",
    "        corpus = pickle.load(f)\n",
    "\n",
    "    # La variable est réapparue\n",
    "    print(\"Corpus brut\", corpus)\n",
    "\n",
    "if not csv_file_exists:\n",
    "    # Création du contenu CSV\n",
    "    nature=corpus.elements_du_corpus()[0]\n",
    "    titre=corpus.elements_du_corpus()[1]\n",
    "    Auteur=corpus.elements_du_corpus()[2]\n",
    "    Co_Auteurs=corpus.elements_du_corpus()[3]\n",
    "    nb_commentaires=corpus.elements_du_corpus()[4]\n",
    "    Date=corpus.elements_du_corpus()[5]\n",
    "    url=corpus.elements_du_corpus()[6]\n",
    "    texte=corpus.elements_du_corpus()[7]\n",
    "    \n",
    "    # Transformation des données en csv\n",
    "    df = pd.DataFrame(zip(nature,titre,Auteur,Co_Auteurs,Date,url,texte,nb_commentaires), columns=['Nature','Titre','Auteur','Co_Auteurs','Date','URL','Texte','Nb commentaires'])\n",
    "    df.to_csv(r'corpus.csv',index=False,sep=';')\n",
    "    \n",
    "    print(\"Creation du CSV OK\")\n",
    "\n",
    "else:\n",
    "    print(\"Le corpus et le fichier CSV existent déjà.\")\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f9e37-b650-4576-9b87-f7fb92c6eef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
